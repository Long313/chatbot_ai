import os, io, json, wave
from datetime import datetime
import streamlit as st
import speech_recognition as sr
from audiorecorder import audiorecorder
from chat_core import configure_openai, reply, DEFAULT_MODEL, GROQ_BASE_URL


st.set_page_config(page_title="AMAX - ChatbotüéôÔ∏è ", page_icon="üí¨", layout="wide")


GRADIENT = "linear-gradient(135deg, #822FFF, #FF35C4)"

CUSTOM_CSS = f"""
<style>
/* N·ªÅn tr·∫Øng, ch·ªØ ƒëen m·∫∑c ƒë·ªãnh */
.stApp {{ background: #ffffff; color: #111827; }}

/* Bong b√≥ng chat t·ªëi gi·∫£n */
.chat-bubble {{
  border-radius: 14px;
  padding: 10px 14px;
  margin: 8px 0;
  max-width: 85%;
  font-size: 15px;
  border: 1px solid #e5e7eb;
  background: #fff;
}}
.user-bubble {{ margin-left:auto; background:#fafafa; }}
.assistant-bubble {{ margin-right:auto; background:#f8fafc; }}

/* Ti√™u ƒë·ªÅ ph·ª•, timestamp */
.timestamp {{ font-size:12px; color:#6b7280; }}

/* N√∫t: n·ªÅn gradient, ch·ªØ tr·∫Øng */
.stButton > button {{
  background: {GRADIENT};
  color: white;
  border: none;
  border-radius: 10px;
  padding: 8px 14px;
}}
.stButton > button:hover {{
  filter: brightness(0.95);
}}

/* Input: vi·ªÅn gradient (gi·ªØ n·ªÅn tr·∫Øng) */
input[type="text"], input[type="password"], textarea {{
  border: 2px solid transparent !important;
  border-radius: 7px !important;
  background-image: linear-gradient(#fff, #fff), {GRADIENT};
  background-origin: border-box;
  background-clip: padding-box, border-box;
}}

/* Chat input (√¥ nh·∫≠p d∆∞·ªõi c√πng) */
[data-testid="stChatInput"] textarea {{
  border: 2px solid transparent !important;
  border-radius: 7px !important;
  background-image: linear-gradient(#fff, #fff), {GRADIENT};
  background-origin: border-box;
  background-clip: padding-box, border-box;
}}

/* Sidebar tr·∫Øng, vi·ªÅn gradient tr√™n input nh∆∞ tr√™n */
[data-testid="stSidebar"] {{
  background: #FFF3FC;
}}


/* Audio player nh·∫π nh√†ng */
.stAudio audio {{ width: 100%; outline: none; }}
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# ---- Ti√™u ƒë·ªÅ ----
st.markdown("<h1 style='text-align:center;'>üí¨ Lu√¥n lu√¥n l·∫Øng nghe...</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center;color:#6b7280;'>Nh·∫≠p vƒÉn b·∫£n ho·∫∑c ghi √¢m ti·∫øng Vi·ªát ƒë·ªÉ h·ªèi</p>", unsafe_allow_html=True)

# ---- Sidebar (ti·∫øng Vi·ªát) ----
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    groq_key = st.text_input("GROQ API Key (gsk_‚Ä¶)", type="password", help="D√°n key t·ª´ console.groq.com")
    base_url = st.text_input("API Base URL", value=GROQ_BASE_URL)
    model = st.text_input("Model", value=DEFAULT_MODEL)
    auto_send = st.checkbox("T·ª± g·ª≠i sau khi nh·∫≠n d·∫°ng gi·ªçng n√≥i", value=True)
    if st.button("üßπ Xo√° h·ªôi tho·∫°i"):
        st.session_state.messages = []

# ---- Tr·∫°ng th√°i phi√™n ----
if "messages" not in st.session_state:
    st.session_state.messages = []

# ---- H√†m hi·ªÉn th·ªã bong b√≥ng ----
def render_message(role, content, t=None):
    t = t or datetime.now().strftime("%H:%M")
    bubble_class = "user-bubble" if role=="user" else "assistant-bubble"
    who = "üßë" if role=="user" else "ü§ñ"
    st.markdown(
        f"""
        <div class="chat-bubble {bubble_class}">
            <div><b>{who}</b> ¬∑ <span class="timestamp">{t}</span></div>
            <div>{content}</div>
        </div>
        """,
        unsafe_allow_html=True
    )

# ---- L·ªãch s·ª≠ chat ----
for m in st.session_state.messages:
    render_message(m["role"], m["content"], m.get("t"))

# ---- Khu nh·∫≠p li·ªáu ----
col_text, col_voice = st.columns([2,1])
with col_text:
    user_text = st.chat_input("Nh·∫≠p tin nh·∫Øn‚Ä¶")

with col_voice:
    #st.subheader("üé§")
    audio = audiorecorder("B·∫•m ƒë·ªÉ n√≥i üé§", "B·∫•m ƒë·ªÉ d·ª´ng ‚úîÔ∏è")
    transcript = None
    if len(audio) > 0:
        # Chuy·ªÉn AudioSegment -> WAV bytes (kh√¥ng c·∫ßn ffmpeg)
        wav_bytes = io.BytesIO()
        with wave.open(wav_bytes, "wb") as wf:
            wf.setnchannels(audio.channels)
            wf.setsampwidth(audio.sample_width)
            wf.setframerate(audio.frame_rate)
            wf.writeframes(audio.raw_data)
        wav_bytes.seek(0)
        st.audio(wav_bytes.getvalue(), format="audio/wav")

        # Nh·∫≠n d·∫°ng ti·∫øng Vi·ªát
        r = sr.Recognizer()
        try:
            with sr.AudioFile(wav_bytes) as source:
                data = r.record(source)
            transcript = r.recognize_google(data, language="vi-VN")
            st.success(f"üìù Nh·∫≠n d·∫°ng: {transcript}")
        except sr.UnknownValueError:
            st.warning("Ch∆∞a nghe r√µ n·ªôi dung. H√£y n√≥i g·∫ßn micro v√† r√µ h∆°n nh√©.")
        except sr.RequestError as e:
            st.error(f"L·ªói d·ªãch v·ª• nh·∫≠n d·∫°ng: {e}")

# ---- G·ª≠i c√¢u h·ªèi ----
to_send = user_text or transcript
if to_send:
    st.session_state.messages.append({"role":"user","content":to_send,"t":datetime.now().strftime("%H:%M")})
    render_message("user", to_send)
    try:
        configure_openai(api_key=groq_key, api_base=base_url)
        bot_text = reply(to_send)
    except Exception as e:
        bot_text = f"‚ö†Ô∏è L·ªói API: {e}"
    st.session_state.messages.append({"role":"assistant","content":bot_text,"t":datetime.now().strftime("%H:%M")})
    render_message("assistant", bot_text)